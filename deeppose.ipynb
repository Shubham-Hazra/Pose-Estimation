{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (LSP)\n",
    "# The dataset is in the form of a dictionary with three keys: 'images', 'keypoints', 'images_visualized'\n",
    "# 'images' is the original image\n",
    "# 'keypoints' is the ground truth keypoints\n",
    "# 'images_visualized' is the original image with the ground truth keypoints visualized\n",
    "# However, the image and visualized image are different in size and the keypoints are not aligned with the original image but with the visualized image\n",
    "# The dataset is split into train and test\n",
    "ds_train = deeplake.load(\"hub://activeloop/lsp-train\")\n",
    "ds_test = deeplake.load(\"hub://activeloop/lsp-test\")\n",
    "\n",
    "# The dataset can be converted to a tensorflow dataset\n",
    "dataloader_train = ds_train.tensorflow()\n",
    "dataloader_test = ds_test.tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the keypoints\n",
    "# The bounding box is in the form of [x,y,w,h] where x and y are the coordinates of the center of the bounding box and w and h are the width and height of the bounding box\n",
    "# This function is the same as the one in the paper\n",
    "def normalize_keypoints(keypoints,bounding_box):\n",
    "    keypoints[:,0] = (keypoints[:,0] - bounding_box[0])/bounding_box[2]\n",
    "    keypoints[:,1] = (keypoints[:,1] - bounding_box[1])/bounding_box[3]\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to unnormalize the keypoints\n",
    "# This function is the same as the one in the paper\n",
    "def unnormalize_keypoints(keypoints,bounding_box):\n",
    "    keypoints[:,0] = (keypoints[:,0]*bounding_box[2]) + bounding_box[0]\n",
    "    keypoints[:,1] = (keypoints[:,1]*bounding_box[3]) + bounding_box[1]\n",
    "    keypoints[:,0] = np.clip(keypoints[:,0],0,bounding_box[2])\n",
    "    keypoints[:,1] = np.clip(keypoints[:,1],0,bounding_box[3])\n",
    "    return keypoints.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the keypoints\n",
    "# image_visualized is needed to get the size of the image on which the keypoints are aligned\n",
    "# This draws the keypoints and the lines connecting the keypoints\n",
    "def visualize_keypoints(image,keypoints,image_visualized):\n",
    "    height,width = image_visualized.shape[:2]\n",
    "    try:\n",
    "        image = cv2.resize(image.numpy(),(width,height))\n",
    "    except:\n",
    "        image = cv2.resize(image,(width,height))\n",
    "    fig , ax = plt.subplots(1,figsize=(10,10))\n",
    "    ax.imshow(image)\n",
    "    ax.scatter(keypoints[:,0],keypoints[:,1])\n",
    "    for i in range(keypoints.shape[0]):\n",
    "        ax.annotate(str(i), (keypoints[i,0],keypoints[i,1]),fontsize=10)\n",
    "    for i in range(0,5):\n",
    "        ax.plot([keypoints[i,0],keypoints[i+1,0]],[keypoints[i,1],keypoints[i+1,1]],linewidth=5)\n",
    "    for i in range(6,11):\n",
    "        ax.plot([keypoints[i,0],keypoints[i+1,0]],[keypoints[i,1],keypoints[i+1,1]],linewidth=5)\n",
    "    for i in range(12,13):\n",
    "        ax.plot([keypoints[i,0],keypoints[i+1,0]],[keypoints[i,1],keypoints[i+1,1]],linewidth=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize on a sample image\n",
    "batch_iter = iter(dataloader_train)\n",
    "batch = next(batch_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image and the keypoints\n",
    "batch = next(batch_iter)\n",
    "image = batch[\"images\"]\n",
    "keypoints = batch[\"keypoints\"]\n",
    "image_visualized = batch[\"images_visualized\"]\n",
    "\n",
    "# Visualize the keypoints\n",
    "visualize_keypoints(image,keypoints,image_visualized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the images and keypoints from the dataset and store them in a numpy array for training\n",
    "# The keypoints are normalized and stored in the keypoints_array\n",
    "# The images are resized to 220x220 and stored in the images array (also normalized to 0-1)\n",
    "images = []\n",
    "keypoints_array = []\n",
    "\n",
    "# Iterate over the training dataset and store the images and keypoints\n",
    "for batch in dataloader_train:\n",
    "    image = batch[\"images\"]\n",
    "    keypoints = batch[\"keypoints\"].numpy().astype(np.float32)\n",
    "    keypoints = keypoints[:,0:2]\n",
    "    image_visualized = batch[\"images_visualized\"].numpy()\n",
    "    height,width = image_visualized.shape[:2]\n",
    "    keypoints = normalize_keypoints(keypoints,(width/2,height/2,width,height))\n",
    "    image = cv2.resize(image.numpy(),(220,220)).astype(np.float32)/255.0\n",
    "    images.append(image)\n",
    "    keypoints_array.append(keypoints)\n",
    "\n",
    "# Since the dataset is small, we can also use the test dataset for training\n",
    "# Iterate over the test dataset and store the images and keypoints\n",
    "for batch in dataloader_test:\n",
    "    image = batch[\"images\"]\n",
    "    keypoints = batch[\"keypoints\"].numpy().astype(np.float32)\n",
    "    keypoints = keypoints[:,0:2]\n",
    "    image_visualized = batch[\"images_visualized\"].numpy()\n",
    "    height,width = image_visualized.shape[:2]\n",
    "    keypoints = normalize_keypoints(keypoints,(width/2,height/2,width,height))\n",
    "    image = cv2.resize(image.numpy(),(220,220)).astype(np.float32)/255.0\n",
    "    images.append(image)\n",
    "    keypoints_array.append(keypoints)\n",
    "\n",
    "# Convert the images and keypoints_array to numpy arrays\n",
    "images = np.array(images)\n",
    "keypoints_array = np.array(keypoints_array,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "# The model is the same as the one in the paper\n",
    "# This is also called the AlexNet model\n",
    "# Use batch normalization for wherever LRN is used in the paper\n",
    "# Default output is 14*2 (14 keypoints with x and y coordinates)\n",
    "def get_model(output = 14*2):\n",
    "    model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(96,(11,11),strides=(4,4),activation=\"relu\",input_shape=(220,220,3)),\n",
    "                                        tf.keras.layers.BatchNormalization(),\n",
    "                                        tf.keras.layers.MaxPool2D((2,2),strides=(2,2)),\n",
    "                                        tf.keras.layers.Conv2D(256,(5,5),activation=\"relu\",padding=\"same\"),\n",
    "                                        tf.keras.layers.BatchNormalization(),\n",
    "                                        tf.keras.layers.MaxPool2D((2,2),strides=(2,2)),\n",
    "                                        tf.keras.layers.Conv2D(384,(3,3),activation=\"relu\"),\n",
    "                                        tf.keras.layers.BatchNormalization(),\n",
    "                                        tf.keras.layers.Conv2D(384,(3,3),activation=\"relu\"),\n",
    "                                        tf.keras.layers.BatchNormalization(),\n",
    "                                        tf.keras.layers.Conv2D(256,(3,3),activation=\"relu\"),\n",
    "                                        tf.keras.layers.BatchNormalization(),\n",
    "                                        tf.keras.layers.MaxPool2D((2,2)),\n",
    "                                        tf.keras.layers.Flatten(),\n",
    "                                        tf.keras.layers.Dense(4096,activation=\"relu\"),\n",
    "                                        tf.keras.layers.Dropout(0.5),\n",
    "                                        tf.keras.layers.Dense(4096,activation=\"relu\"),\n",
    "                                        tf.keras.layers.Dropout(0.5),\n",
    "                                        tf.keras.layers.Dense(output,activation=\"linear\")])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom loss function is used\n",
    "# The loss function is the same as the one in the paper (L2 loss)\n",
    "# The loss function is the sum of the squared difference between the predicted and ground truth keypoints\n",
    "# Reshape the predicted keypoints(batch_size,28) to (batch_size,14,2) before calculating the loss\n",
    "def loss_fn(y_true,y_pred):\n",
    "    y_pred = tf.reshape(y_pred,(-1,14,2))\n",
    "    return tf.reduce_sum(tf.square(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for a sample image\n",
    "sample = images[0]\n",
    "sample = cv2.resize(sample,(220,220)).astype(np.float32)/255.0\n",
    "sample = np.expand_dims(sample,axis=0)\n",
    "sample = tf.convert_to_tensor(sample,dtype=tf.float32)\n",
    "pred = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss\n",
    "loss_fn(keypoints_array[0],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer and the custom loss function\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# model.fit(images,keypoints_array,epochs=10,batch_size=2,validation_split=0.1,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# model.save(\"deeppose.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = tf.keras.models.load_model(\"deeppose.h5\",custom_objects={\"loss_fn\":loss_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the predictions from the model for a given image\n",
    "def get_preds(model,image):\n",
    "    image= cv2.resize(image,(220,220))\n",
    "    image = np.expand_dims(image,axis=0)\n",
    "    image = tf.convert_to_tensor(image,dtype=tf.float32)\n",
    "    pred = model(image)\n",
    "    pred = tf.reshape(pred,(14,2))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a sample image\n",
    "test_iter = iter(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions\n",
    "# We will see that the model performs resonably well on our dataset\n",
    "img = next(test_iter)[\"images\"].numpy()\n",
    "img = cv2.resize(img,(220,220))\n",
    "image = cv2.resize(img,(220,220)).astype(np.float32)/255.0\n",
    "pred = get_preds(model,image).numpy()\n",
    "keypoints = unnormalize_keypoints(pred,(110,110,220,220))\n",
    "visualize_keypoints(img,keypoints,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "# We can see that the model is not able to generalize well\n",
    "img = cv2.imread(\"download.jfif\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img,(220,220))\n",
    "image = cv2.resize(img,(220,220)).astype(np.float32)/255.0\n",
    "pred = get_preds(model,image).numpy()\n",
    "keypoints = unnormalize_keypoints(pred,(110,110,220,220))\n",
    "visualize_keypoints(img,keypoints,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we shall move on to train the cascaded model\n",
    "# The cascaded model is the same as the one in the paper\n",
    "# The cascaded model is trained on the same dataset as the original model\n",
    "# The cascaded model is trained on the difference between the predicted and ground truth keypoints\n",
    "# Unnormalize the keypoints before training\n",
    "for i in range(keypoints_array.shape[0]):\n",
    "    keypoints_array[i] = unnormalize_keypoints(keypoints_array[i],(110,110,220,220)).astype(np.int32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the diameter \n",
    "# The diameter is the distance between the left shoulder and right hip\n",
    "diams = []\n",
    "for i,image in enumerate(images):\n",
    "    diams.append(np.sqrt(np.sum(np.square(keypoints_array[i][8]-keypoints_array[i][3]))))\n",
    "diams = np.array(diams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplier to multiply the diameter to get the bounding box size\n",
    "SIGMA = 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the predicted keypoints from the original model\n",
    "# preds = model(images)\n",
    "# preds = tf.reshape(preds,(-1,14,2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalize the predicted keypoints\n",
    "for i in range(preds.shape[0]):\n",
    "    preds[i] = unnormalize_keypoints(preds[i],(110,110,220,220))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the difference between the predicted and ground truth keypoints\n",
    "keypoints_errors = keypoints_array - preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the training dataset for the cascaded model\n",
    "def crop_and_resize(image,bounding_box):\n",
    "    x,y,w,h = bounding_box\n",
    "    top_left_x = int(max(0,x-(w//2)))\n",
    "    top_left_y = int(max(0,y-(h//2)))\n",
    "    bottom_right_x = int(min(image.shape[1],x+(w//2)))\n",
    "    bottom_right_y = int(min(image.shape[0],y+(h//2)))\n",
    "    image = image[top_left_y:bottom_right_y,top_left_x:bottom_right_x]\n",
    "    image = cv2.resize(image,(220,220))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the bounding boxes corresponding to each keypoint\n",
    "bounding_boxes = []\n",
    "for i in range(keypoints_array.shape[0]):\n",
    "    bounding_box_per_image = []\n",
    "    for k in range(14):\n",
    "        x,y = keypoints_array[i][k]\n",
    "        bounding_box_per_image.append([x,y,SIGMA*diams[i]+0.0001,SIGMA*diams[i]+0.0001])\n",
    "    bounding_box_per_image = np.array(bounding_box_per_image)\n",
    "    bounding_boxes.append(bounding_box_per_image)\n",
    "bounding_boxes = np.array(bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the keypoints_errors\n",
    "for i in range(keypoints_errors.shape[0]):\n",
    "    for k in range(keypoints_errors.shape[2]):\n",
    "        keypoints_errors[i,k,:] = normalize_keypoints(np.array([keypoints_errors[i,k,:]]),bounding_boxes[i,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cascaded model (same as the one in the paper)\n",
    "# Has 2 outputs: displacement in x and y direction\n",
    "cascade_model = get_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "# The loss function is the same as the one in the paper (L2 loss)\n",
    "# The loss function is the sum of the squared difference between the predicted and ground truth keypoints\n",
    "def loss_fn_cascade(y_true,y_pred):\n",
    "    return tf.reduce_sum(tf.square(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom training step\n",
    "@tf.function\n",
    "def train_step(image,keypoints_errors):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = cascade_model(image)\n",
    "        loss_value = loss_fn_cascade(keypoints_errors,pred)\n",
    "    grads = tape.gradient(loss_value,cascade_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads,cascade_model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Function to train the model\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        loss_value = 0\n",
    "        for i in range(images.shape[0]):\n",
    "            for k in range(14):\n",
    "                image = crop_and_resize(images[i],bounding_boxes[i,k])\n",
    "                image = np.expand_dims(image,axis=0)\n",
    "                image = tf.convert_to_tensor(image,dtype=tf.float32)\n",
    "                keypoints_error = keypoints_errors[i,k]\n",
    "                keypoints_error = np.expand_dims(keypoints_error,axis=0)\n",
    "                keypoints_error = tf.convert_to_tensor(keypoints_error,dtype=tf.float32)\n",
    "                loss_value += train_step(image,keypoints_error)\n",
    "            if(i%100==0):\n",
    "                print(f\"{i} images done with loss {loss_value.numpy()} for epoch {epoch}\")\n",
    "        print(f\"Epoch {epoch} done with loss {loss_value.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# train(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# cascade_model.save(\"deeppose_cascade_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "cascade_model = tf.keras.models.load_model(\"deeppose_cascade_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a sample image\n",
    "test_iter = iter(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions on a sample image\n",
    "# We can see that the model performs resonably well on our dataset\n",
    "# We refine the predictions from the original model using the cascaded model\n",
    "img = next(test_iter)[\"images\"].numpy()\n",
    "img = cv2.resize(img,(220,220))\n",
    "image = cv2.resize(img,(220,220)).astype(np.float32)/255.0\n",
    "pred = get_preds(model,image).numpy()\n",
    "keypoints = unnormalize_keypoints(pred,(110,110,220,220))\n",
    "sigma = 1.25\n",
    "diam = np.sqrt(np.sum(np.square(keypoints[8]-keypoints_array[3])))\n",
    "w = int(sigma*diam + 0.0001)\n",
    "h = int(sigma*diam + 0.0001)\n",
    "for i in range(14):\n",
    "    image = crop_and_resize(img/255.0,(keypoints[i,0],keypoints[i,1],w,h))\n",
    "    image = np.expand_dims(image,axis=0)\n",
    "    image = tf.convert_to_tensor(image,dtype=tf.float32)\n",
    "    pred = cascade_model(image)\n",
    "    pred = tf.reshape(pred,(2,))\n",
    "    # pred = unnormalize_keypoints(np.array([pred.numpy()]),(keypoints[i,0],keypoints[i,1],w,h))\n",
    "    keypoints[i] += pred[0]\n",
    "visualize_keypoints(img,keypoints,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "img = cv2.imread(\"download.jfif\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img,(220,220))\n",
    "image = cv2.resize(img,(220,220)).astype(np.float32)/255.0\n",
    "pred = get_preds(model,image).numpy()\n",
    "keypoints = unnormalize_keypoints(pred,(110,110,220,220))\n",
    "sigma = 1.25\n",
    "diam = np.sqrt(np.sum(np.square(keypoints[8]-keypoints_array[3])))\n",
    "w = int(sigma*diam + 0.0001)\n",
    "h = int(sigma*diam + 0.0001)\n",
    "for i in range(14):\n",
    "    image = crop_and_resize(img/255.0,(keypoints[i,0],keypoints[i,1],w,h))\n",
    "    image = np.expand_dims(image,axis=0)\n",
    "    image = tf.convert_to_tensor(image,dtype=tf.float32)\n",
    "    pred = cascade_model(image)\n",
    "    pred = tf.reshape(pred,(2,))\n",
    "    # pred = unnormalize_keypoints(np.array([pred.numpy()]),(keypoints[i,0],keypoints[i,1],w,h))\n",
    "    keypoints[i] += pred[0]\n",
    "visualize_keypoints(img,keypoints,img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
